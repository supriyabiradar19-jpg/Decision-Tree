{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScMQb0EOaEv0"
      },
      "outputs": [],
      "source": [
        "1.  What is a Decision Tree, and how does it work in the context of classification?\n",
        "   -> A Decision Tree is a popular and intuitive supervised learning algorithm used for\n",
        "      both classification and regression tasks. In the context of classification, a decision tree\n",
        "      helps to assign input data into predefined classes by learning decision rules from the features of the data.\n",
        "\n",
        "      # Here's a breakdown of how it works:\n",
        "  # 1. Tree Structure:\n",
        "# Root Node:\n",
        "   The starting point of the tree, representing the entire dataset.\n",
        "Internal Nodes:\n",
        "   Represent tests or questions about specific features of the data.\n",
        "# Branches:\n",
        "   Represent the outcomes of the tests at internal nodes, leading to further nodes.\n",
        "# Leaf Nodes:\n",
        "    The end of the branches, representing the final classification or prediction for a given instance.\n",
        "# 2. Classification Process:\n",
        "#  Recursive Splitting:\n",
        "       The algorithm starts at the root node and selects the \"best\" feature to split the data based on\n",
        "        a chosen criterion (e.g., information gain, Gini index).\n",
        "#  Branching:\n",
        "         Based on the chosen feature's value, the data is split into different branches, each representing a\n",
        "          possible outcome of the feature test.\n",
        "#  Repeating the Process:\n",
        "       This splitting process is repeated recursively for each branch until certain stopping criteria are met\n",
        "         (e.g., reaching a maximum tree depth, all instances in a node belong to the same class).\n",
        "#   Classification at Leaf Nodes:\n",
        "              Once a leaf node is reached, the instance is classified into the class label associated with that leaf.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2.  Explain the concepts of Gini Impurity and Entropy as impurity measures.How do they impact the splits in a Decision Tree?\n",
        "   ->\n",
        "  An impurity measure quantifies how mixed or impure a node is in terms of class distribution.\n",
        "   The goal is to minimize impurity with every split—leading to leaf nodes that ideally contain examples from only one class.\n",
        "\n",
        "  #  Gini Impurity:\n",
        "        *  It quantifies the likelihood of misclassifying a randomly selected element in a node\n",
        "            if it were labeled based on the node's class distribution.\n",
        "        *  Gini impurity is calculated using the formula: Gini = 1 - Σ(p\\_i)^2, where p\\_i\n",
        "            is the proportion of instances of class i in the node.\n",
        "       *  A Gini score of 0 indicates a perfectly pure node (all instances belong to the same\n",
        "              class), while a score of 0.5 represents maximum impurity (an equal distribution of classes).\n",
        "\n",
        "\n",
        "# Entropy:\n",
        "          *  Entropy measures the randomness or uncertainty in a dataset. It's based on information theory\n",
        "              reflecting the amount of information needed to describe the class distribution.\n",
        "         *  A lower entropy value indicates higher purity, while a higher entropy value signifies greater impurity.\n",
        "        *  Entropy is calculated using the formula: Entropy = - Σ(p\\_i * log2(p\\_i)), where p\\_i is the proportion\n",
        "            of instances of class i in the node.\n",
        "\n",
        "# Impact on Decision Tree Splits:\n",
        "     *  Both Gini Impurity and Entropy are used as criteria to evaluate potential splits in a decision tree.\n",
        "     *  The algorithm calculates the impurity of the current node (parent node) and then evaluates potential splits based on different features.\n",
        "     *  For each split, the weighted average impurity of the resulting child nodes is calculated.\n",
        "     *  The split that results in the lowest impurity (either lowest Gini or lowest entropy) is selected as the best split for that node.\n",
        "\n",
        "# Key Differences:\n",
        "     *  While both measures aim to reduce impurity, Gini impurity is generally faster to compute because it doesn't involve logarithmic calculations.\n",
        "     *  Entropy is considered more sensitive to class distribution and can lead to slightly deeper and more balanced trees.\n",
        "\n"
      ],
      "metadata": {
        "id": "SlFm-5msgnFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3.  What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "   ->  . Pre-pruning stops the tree from growing during its construction, while post-pruning trims a fully grown tree.\n",
        "\n",
        "# Definition:\n",
        "              Pre-pruning involves setting conditions (e.g., minimum number of samples required for a split,\n",
        "              maximum depth of the tree) during the tree's construction that prevent it from growing further if these conditions are not met.\n",
        "\n",
        "# Practical Advantage:\n",
        "    Faster training because the tree is not fully grown, saving computational resources.\n",
        "# Example:\n",
        "    If the stopping criterion is a minimum number of samples in a node, the tree will stop splitting a node\n",
        "     further when the number of samples in that node is less than the specified minimum.\n",
        "\n",
        "# Post-Pruning (Backward Pruning):\n",
        "\n",
        "# Definition:\n",
        "Post-pruning involves first building a complete decision tree and then pruning it back by removing branches or\n",
        " subtrees that do not significantly contribute to the model's accuracy.\n",
        "# Practical Advantage:\n",
        "Allows for a more thorough evaluation of the tree's structure because it considers all possible\n",
        " splits before pruning, potentially leading to better generalization.\n",
        "# Example:\n",
        " A subtree might be pruned if its removal results in a negligible decrease in accuracy on\n",
        " a validation set, but a significant reduction in the tree's size.\n",
        "\n"
      ],
      "metadata": {
        "id": "1g31vZRRh6LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4.  What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "    ->  Information gain is a metric used in decision tree algorithms to determine the best way to split data at each node.\n",
        "      Elaboration:\n",
        "# Entropy:\n",
        "    Entropy is a measure of impurity or randomness in a dataset. It quantifies how mixed the target variable's values are within a node.\n",
        "# Information Gain Calculation:\n",
        "   Information gain is calculated by subtracting the weighted average entropy of the child nodes from the entropy of the parent node.\n",
        "# Formula:\n",
        "    Gain(S, A) = Entropy(S) - Σ (|Sv| / |S|) * Entropy(Sv)\n",
        "*   S is the parent node dataset.\n",
        "*   A is the attribute used for splitting.\n",
        "*   Sv is a subset of S created by splitting on attribute A.\n",
        "\n",
        "#  Why it's Important:\n",
        "    Decision trees aim to create nodes that are as pure as possible (i.e., contain mostly data points belonging to the same class).\n",
        "  Information gain helps identify the attributes that best achieve this purity by reducing entropy the most.\n",
        "#  Best Split:\n",
        "  The attribute with the highest information gain is selected as the best split because it\n",
        "  provides the most significant reduction in uncertainty and leads to more informative child nodes.\n"
      ],
      "metadata": {
        "id": "ZCz8qvLSi_JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5.   What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "    #  ->  Common Real-World Applications:\n",
        "# Banking:\n",
        "   Used for loan approval by assessing factors like credit score, income, and employment history.\n",
        "# Healthcare:\n",
        "    Aid in disease diagnosis (e.g., diabetes) based on patient data like glucose levels and blood pressure.\n",
        "# Marketing:\n",
        "    Help with customer segmentation and targeting to optimize marketing strategies.\n",
        "# Finance:\n",
        "   Used for credit scoring and risk assessment, aiding in lending decisions.\n",
        "# Retail:\n",
        "    Used for inventory management by predicting sales trends.\n",
        "# Telecommunications:\n",
        "   Identify customer churn by analyzing usage patterns and predicting cancellations.\n",
        "# Manufacturing:\n",
        "    Optimize production processes by analyzing factors affecting efficiency.\n",
        "\n",
        "# Advantages:\n",
        "\n",
        "# Easy to understand and interpret:\n",
        "       Decision trees are visually represented as flowcharts, making them easy to follow and explain, even to non-experts.\n",
        "#  Handles both numerical and categorical data:\n",
        "       They don't require extensive data preprocessing or encoding of different data types.\n",
        "#  Captures non-linear relationships:\n",
        "    Decision trees can effectively model complex patterns and relationships within the data.\n",
        "#  Automated feature selection:\n",
        "    They can automatically identify the most relevant features for decision-making.\n",
        "#  Minimal data preparation:\n",
        "   Decision trees often require less data preprocessing compared to some other algorithms.\n",
        "\n",
        "# Disadvantages:\n",
        "\n",
        "#  Overfitting:\n",
        "     Decision trees can easily overfit the training data, especially if they are too deep or complex,\n",
        "       leading to poor generalization on unseen data.\n",
        "#   Sensitivity to data changes:\n",
        "    Small changes in the training data can lead to significant changes in the tree structure.\n",
        "#  Bias towards dominant classes:\n",
        "     Decision trees can be biased towards the majority class in imbalanced datasets.\n"
      ],
      "metadata": {
        "id": "gNOnvuyKj-t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "# criterion='gini' is the default, but explicitly set for clarity\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, dt_classifier.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iPqZ7fkgnTe",
        "outputId": "3c12f22e-635e-40f2-b1b9-8c586f8c2115"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "7.  Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth=3\n",
        "dt_constrained = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_constrained.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set for the constrained tree\n",
        "y_pred_constrained = dt_constrained.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the constrained tree\n",
        "accuracy_constrained = accuracy_score(y_test, y_pred_constrained)\n",
        "print(f\"Accuracy of Decision Tree with max_depth=3: {accuracy_constrained:.4f}\")\n",
        "\n",
        "# Train a fully-grown Decision Tree Classifier (no max_depth constraint)\n",
        "dt_full = DecisionTreeClassifier(random_state=42)\n",
        "dt_full.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set for the fully-grown tree\n",
        "y_pred_full = dt_full.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the fully-grown tree\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Accuracy of fully-grown Decision Tree: {accuracy_full:.4f}\")\n",
        "\n",
        "# Compare the accuracies\n",
        "if accuracy_full > accuracy_constrained:\n",
        "    print(\"\\nThe fully-grown tree has higher accuracy.\")\n",
        "elif accuracy_constrained > accuracy_full:\n",
        "    print(\"\\nThe tree with max_depth=3 has higher accuracy.\")\n",
        "else:\n",
        "    print(\"\\nBoth trees have the same accuracy.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqlpb89bgnVK",
        "outputId": "ecf24687-609e-413e-be43-089399771194"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree with max_depth=3: 1.0000\n",
            "Accuracy of fully-grown Decision Tree: 1.0000\n",
            "\n",
            "Both trees have the same accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "8.   Write a Python program to:\n",
        "● Load the California Housing dataset from sklearn\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing(as_frame=True)\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "# You can adjust max_depth to control the complexity of the tree and prevent overfitting\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42, max_depth=8)\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = dt_regressor.feature_importances_\n",
        "feature_names = X.columns\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(importance_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwgR9ozmgnYr",
        "outputId": "200e4e06-6668-4b93-d647-feb70f30927a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.4220\n",
            "\n",
            "Feature Importances:\n",
            "      Feature  Importance\n",
            "0      MedInc    0.662933\n",
            "5    AveOccup    0.132096\n",
            "6    Latitude    0.061311\n",
            "7   Longitude    0.050202\n",
            "1    HouseAge    0.042222\n",
            "2    AveRooms    0.034092\n",
            "3   AveBedrms    0.008931\n",
            "4  Population    0.008214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "9. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Define the Decision Tree Classifier and parameter grid for GridSearchCV\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for max_depth and min_samples_split\n",
        "param_grid = {\n",
        "    'max_depth': [None, 3, 5, 7, 10],  # None means no limit on depth\n",
        "    'min_samples_split': [2, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# cv=5 for 5-fold cross-validation\n",
        "# scoring='accuracy' to evaluate models based on accuracy\n",
        "grid_search = GridSearchCV(estimator=dtree, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 3. Print the best parameters and the resulting model accuracy\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Get the best estimator (model) found by GridSearchCV\n",
        "best_dtree_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_dtree_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the best model on the test set: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn2SGgeVgnbl",
        "outputId": "f3ca2b72-c2b5-495b-fd9a-c0031f3d1916"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found by GridSearchCV:\n",
            "{'max_depth': None, 'min_samples_split': 10}\n",
            "Accuracy of the best model on the test set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "10.: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "\n",
        "# ->  Step-by-step Process:\n",
        "\n",
        "\n",
        "# 1. Handling Missing Values:\n",
        "Identify Missing Data:\n",
        "    Determine the extent and pattern of missing values in the dataset.\n",
        "Choose Imputation Method:\n",
        "Numerical Features:\n",
        "      Use mean, median, or mode based on data distribution and the presence of outliers. For instance,\n",
        "        the median might be preferred if there are outliers in the numerical data. More advanced techniques like\n",
        "         KNN imputation can also be employed to estimate missing values based on similar data points.\n",
        "Categorical Features:\n",
        "     Fill missing values with the most frequent category (mode) or create a new category specifically for missing values.\n",
        "Apply Imputation:\n",
        "     Replace the missing values with the chosen estimations using appropriate functions (e.g., fillna() in pandas).\n",
        "# 2. Encoding Categorical Features:\n",
        "Identify Categorical Columns:\n",
        "       Determine which columns in the dataset contain categorical data (e.g., gender, disease type).\n",
        "# Choose Encoding Technique:\n",
        "One-Hot Encoding:\n",
        "      Creates a new binary column for each category in a feature, suitable when there's no inherent order\n",
        "        among categories. For example, a \"color\" feature with values \"red\", \"blue\", \"green\" would be transformed into three binary columns.\n",
        "Label Encoding:\n",
        "      Assigns a unique integer to each category. This is useful when there is an ordina\n",
        "       l relationship between categories. For instance, \"small\", \"medium\", and \"large\" could be encoded as 0, 1, and 2, respectively.\n",
        "# Apply Encoding:\n",
        "     Use libraries like scikit-learn to perform the chosen encoding.\n",
        "# 3. Training the Decision Tree Model:\n",
        "Prepare Data:\n",
        "       Split the dataset into training and testing sets to evaluate the model's performance on unseen data.\n",
        "Instantiate the Model:\n",
        "     Create a Decision Tree Classifier object from the DecisionTreeClassifier class in scikit-learn.\n",
        "Fit the Model:\n",
        "      Train the model on the training data using the fit() method.\n",
        "# 4. Tuning Hyperparameters:\n",
        "Identify Hyperparameters:\n",
        "        Decision Trees have various hyperparameters like max_depth (maximum depth of the tree), min_samples_split\n",
        "        (minimum number of samples required to split an internal node), and min_samples_leaf (minimum number of samples required to be at a leaf node).\n",
        "# Choose Tuning Method:\n",
        "Grid Search:\n",
        "     Systematically explores all possible combinations of hyperparameter values within a defined range.\n",
        "      It is computationally expensive but guarantees finding the optimal combination. according to scikit-learn documentation.\n",
        "# Randomized Search:\n",
        "     Randomly samples hyperparameter combinations, often more efficient than Grid Search when dealing\n",
        "      with a large hyperparameter space. according to scikit-learn documentation.\n",
        "# Optimize:\n",
        "      Use the chosen method to find the best hyperparameter values that maximize the model's performance on a validation set.\n"
      ],
      "metadata": {
        "id": "E0ET3Dwdnifd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}